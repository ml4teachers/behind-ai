# Verwendung eines alternativen Modells für Next-Token-Prediction mit Logprobs

Sie haben recht - `distilgpt2` scheint keine Logprobs zurückzugeben, was für unsere interaktive Demonstration wesentlich ist. Lassen Sie uns auf ein Modell umsteigen, das Logprobs unterstützt. Hugging Face bietet verschiedene Modelle an, die mit logprobs funktionieren.

## Schritt 1: API-Route mit einem Modell aktualisieren, das Logprobs unterstützt

**src/app/api/predict-next/route.ts** (aktualisieren)
```typescript
import { NextResponse } from 'next/server';
import { HfInference } from '@huggingface/inference';

// Hugging Face Inference Client mit API-Key aus Umgebungsvariablen
const hf = new HfInference(process.env.HUGGINGFACE_API_KEY);

// Modell, das Logprobs zurückgibt (gpt2)
const MODEL_ID = "gpt2";

export async function POST(request: Request) {
  try {
    const { text } = await request.json();
    console.log("API empfangener Text:", text);
    
    if (!text || typeof text !== 'string') {
      return NextResponse.json(
        { error: 'Text muss ein gültiger String sein' },
        { status: 400 }
      );
    }

    console.log("Rufe Hugging Face API auf mit Modell:", MODEL_ID);
    
    // Textgenerierung mit Wahrscheinlichkeitsberechnung
    // GPT-2 unterstützt logprobs
    const response = await hf.textGeneration({
      model: MODEL_ID,
      inputs: text,
      parameters: {
        max_new_tokens: 1,
        return_full_text: false,
        temperature: 1.0,
        top_k: 50,
        return_logprobs: true,
      }
    });
    
    console.log("API-Response erhalten:", JSON.stringify(response, null, 2));

    // Extrahieren der logprobs aus der Antwort
    let topTokens = [];
    let remainingProbability = 1;
    
    // Prüfen, ob logprobs in der Antwort enthalten sind
    if (response.details && 
        response.details.logprobs && 
        response.details.logprobs.topLogprobs && 
        response.details.logprobs.topLogprobs[0]) {
      
      const topLogprobs = response.details.logprobs.topLogprobs[0];
      console.log("Extrahierte Top-Logprobs:", topLogprobs);
      
      // Umwandlung in ein Array von {token, probability}
      topTokens = Object.entries(topLogprobs).map(([token, logprob]) => ({
        token: token.replace(/Ġ/g, " "), // Bessere Lesbarkeit von GPT-2 speziellen Präfixen
        probability: Math.exp(Number(logprob)),
      }));
      
      console.log("Token-Wahrscheinlichkeiten:", topTokens);
      
      // Sortieren nach Wahrscheinlichkeit (absteigend)
      topTokens = topTokens.sort((a, b) => b.probability - a.probability);
      
      // Top 6 Token auswählen, wenn mehr vorhanden sind
      if (topTokens.length > 6) {
        topTokens = topTokens.slice(0, 6);
      }
      
      // Summe der Wahrscheinlichkeiten berechnen
      const topTokensSum = topTokens.reduce((sum, item) => sum + item.probability, 0);
      remainingProbability = Math.max(0, 1 - topTokensSum);
    } else {
      console.log("Keine logprobs in der Antwort gefunden, verwende Fallback");
      // Fallback - simulieren Sie logprobs, wenn keine vorhanden sind
      topTokens = getFallbackPredictions(text);
    }

    const response_data = {
      text,
      topTokens,
      remainingProbability,
      generatedToken: response.generated_text,
      totalTokens: topTokens.length,
      // Info, wenn wir Fallback-Daten verwenden
      apiNotice: topTokens.length === 0 ? 
        "Modell konnte keine Token-Wahrscheinlichkeiten liefern, zeige Fallback-Daten." : 
        undefined
    };
    
    console.log("Sende zurück an Client:", response_data);
    return NextResponse.json(response_data);
  } catch (error) {
    console.error('Next-Token Prediction Fehler:', error);
    
    // Fallback-Vorhersagen bereitstellen, wenn die API nicht erreichbar ist
    const fallbackTokens = getFallbackPredictions(text);
    
    return NextResponse.json({
      text: text,
      topTokens: fallbackTokens,
      remainingProbability: 0.33,
      generatedToken: fallbackTokens[0].token,
      totalTokens: fallbackTokens.length,
      apiNotice: error instanceof Error 
        ? `API-Fehler: ${error.message}. Zeige Fallback-Daten.` 
        : "API nicht erreichbar. Zeige Fallback-Daten."
    });
  }
}

// Verbesserte Fallback-Funktion mit kontextbezogenen Vorhersagen
function getFallbackPredictions(text: string) {
  const lowercaseText = text.toLowerCase();
  
  if (lowercaseText.includes('sonne scheint')) {
    return [
      { token: ' hell', probability: 0.32 },
      { token: ' heute', probability: 0.24 },
      { token: ' am', probability: 0.14 },
      { token: ' durch', probability: 0.08 },
      { token: ' und', probability: 0.06 },
      { token: ' auf', probability: 0.05 },
    ];
  } else if (lowercaseText.includes('künstliche intelligenz')) {
    return [
      { token: ' ist', probability: 0.28 },
      { token: ' kann', probability: 0.22 },
      { token: ' hat', probability: 0.12 },
      { token: ' wird', probability: 0.09 },
      { token: ' und', probability: 0.07 },
      { token: ' revolutioniert', probability: 0.04 },
    ];
  } else if (lowercaseText.includes('schüler lernen')) {
    return [
      { token: ' mit', probability: 0.26 },
      { token: ' in', probability: 0.18 },
      { token: ' durch', probability: 0.14 },
      { token: ' besser', probability: 0.11 },
      { token: ' schneller', probability: 0.08 },
      { token: ' gemeinsam', probability: 0.05 },
    ];
  } else {
    // Allgemeiner Fallback
    return [
      { token: ' und', probability: 0.18 },
      { token: ' ist', probability: 0.15 },
      { token: ' in', probability: 0.12 },
      { token: ' der', probability: 0.09 },
      { token: ' mit', probability: 0.07 },
      { token: ' die', probability: 0.06 },
    ];
  }
}
```

## Schritt 2: NextTokenPrediction-Komponente verbessern, um Leertoken-Arrays zu behandeln

**src/components/visualizations/next-token-prediction.tsx** (vollständige Komponente)
```tsx
'use client'

import { useState, useEffect } from 'react'
import { motion, AnimatePresence } from 'framer-motion'
import { Button } from "@/components/ui/button"
import { Progress } from '@/components/ui/progress'
import { Skeleton } from "@/components/ui/skeleton"
import { ChevronRightIcon, ShuffleIcon, ReloadIcon } from "@radix-ui/react-icons"

interface NextTokenPredictionProps {
  text: string
  useSimulation?: boolean
}

interface TokenProbability {
  token: string
  probability: number
}

export function NextTokenPrediction({ text, useSimulation = false }: NextTokenPredictionProps) {
  const [loading, setLoading] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [topTokens, setTopTokens] = useState<TokenProbability[]>([])
  const [selectedToken, setSelectedToken] = useState<string | null>(null)
  const [remainingProbability, setRemainingProbability] = useState(0)
  const [currentText, setCurrentText] = useState(text)
  const [predictionHistory, setPredictionHistory] = useState<string[]>([])
  const [apiNotice, setApiNotice] = useState<string | null>(null)

  const fetchNextTokenPrediction = async (inputText: string) => {
    console.log("Start fetchNextTokenPrediction mit Text:", inputText);
    setLoading(true)
    setError(null)
    setSelectedToken(null)
    
    try {
      // Direkt simulierte Daten liefern, wenn Simulationsmodus aktiv ist
      if (useSimulation) {
        console.log("Verwende Simulationsmodus ohne API-Aufruf");
        await new Promise(resolve => setTimeout(resolve, 800)); // Verzögerung für besseres UX
        
        // Simulierte Daten basierend auf Eingabetext generieren
        const simulatedResponse = getSimulatedPredictions(inputText);
        
        console.log("Simulierte Daten:", simulatedResponse);
        setTopTokens(simulatedResponse.topTokens);
        setRemainingProbability(simulatedResponse.remainingProbability);
        setApiNotice("Simulationsmodus aktiv - keine echte API verwendet");
        setLoading(false);
        return;
      }
      
      console.log("Sende Anfrage an API...");
      const response = await fetch('/api/predict-next', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text: inputText }),
      })
      
      console.log("API-Antwort Status:", response.status);
      
      if (!response.ok) {
        throw new Error(`API-Anfrage fehlgeschlagen: ${response.status}`);
      }
      
      const data = await response.json();
      console.log("API-Antwort Daten:", data);
      
      if (data.error) {
        throw new Error(data.error);
      }
      
      // API-Hinweis setzen, falls vorhanden
      if (data.apiNotice) {
        console.log("API-Hinweis gesetzt:", data.apiNotice);
        setApiNotice(data.apiNotice);
      } else {
        setApiNotice(null);
      }
      
      console.log("Setze topTokens:", data.topTokens || []);
      setTopTokens(data.topTokens || []);
      
      console.log("Setze remainingProbability:", data.remainingProbability || 0);
      setRemainingProbability(data.remainingProbability || 0);

      // Wenn keine Tokens zurückgegeben wurden, aber auch kein Fehler vorliegt,
      // zeigen wir eine entsprechende Nachricht an
      if (!data.topTokens || data.topTokens.length === 0) {
        setApiNotice("Keine Token-Wahrscheinlichkeiten vom Modell erhalten. Versuche es mit einem anderen Text.");
      }
    } catch (err) {
      console.error("Fehler in fetchNextTokenPrediction:", err);
      setError(err instanceof Error ? err.message : 'Unbekannter Fehler');
    } finally {
      console.log("fetchNextTokenPrediction abgeschlossen, setze loading=false");
      setLoading(false);
    }
  }

  // Funktion für simulierte Daten
  const getSimulatedPredictions = (text: string) => {
    const lowercaseText = text.toLowerCase();
    
    let topTokens;
    if (lowercaseText.includes('sonne scheint')) {
      topTokens = [
        { token: ' hell', probability: 0.32 },
        { token: ' heute', probability: 0.24 },
        { token: ' am', probability: 0.14 },
        { token: ' durch', probability: 0.08 },
        { token: ' und', probability: 0.06 },
        { token: ' auf', probability: 0.05 },
      ];
    } else if (lowercaseText.includes('künstliche intelligenz')) {
      topTokens = [
        { token: ' ist', probability: 0.28 },
        { token: ' kann', probability: 0.22 },
        { token: ' hat', probability: 0.12 },
        { token: ' wird', probability: 0.09 },
        { token: ' und', probability: 0.07 },
        { token: ' revolutioniert', probability: 0.04 },
      ];
    } else if (lowercaseText.includes('schüler lernen')) {
      topTokens = [
        { token: ' mit', probability: 0.26 },
        { token: ' in', probability: 0.18 },
        { token: ' durch', probability: 0.14 },
        { token: ' besser', probability: 0.11 },
        { token: ' schneller', probability: 0.08 },
        { token: ' gemeinsam', probability: 0.05 },
      ];
    } else {
      // Fallback für andere Texte
      topTokens = [
        { token: ' und', probability: 0.18 },
        { token: ' ist', probability: 0.15 },
        { token: ' in', probability: 0.12 },
        { token: ' der', probability: 0.09 },
        { token: ' mit', probability: 0.07 },
        { token: ' die', probability: 0.06 },
      ];
    }
    
    // Summe der Wahrscheinlichkeiten berechnen
    const sum = topTokens.reduce((acc, token) => acc + token.probability, 0);
    const remainingProbability = Math.max(0, 1 - sum);
    
    return {
      topTokens,
      remainingProbability,
    };
  }

  // Initial und bei Text-Änderungen Vorhersage laden
  useEffect(() => {
    setCurrentText(text)
    setPredictionHistory([])
    fetchNextTokenPrediction(text)
  }, [text, useSimulation]);

  // Zufälligen Token basierend auf Wahrscheinlichkeiten auswählen
  const selectRandomToken = () => {
    if (topTokens.length === 0) return
    
    // Zufallszahl zwischen 0 und 1 generieren
    const random = Math.random()
    let cumulativeProbability = 0
    
    // Durch die Tokens iterieren und basierend auf Wahrscheinlichkeit auswählen
    for (const tokenData of topTokens) {
      cumulativeProbability += tokenData.probability
      if (random <= cumulativeProbability) {
        selectToken(tokenData.token)
        return
      }
    }
    
    // Fallback: ersten Token nehmen, falls alle anderen nicht ausgewählt wurden
    selectToken(topTokens[0].token)
  }

  // Token auswählen und zum Text hinzufügen
  const selectToken = (token: string) => {
    setSelectedToken(token)
    
    // Kurz warten, damit Animation sichtbar ist
    setTimeout(() => {
      const newText = currentText + token
      setCurrentText(newText)
      setPredictionHistory([...predictionHistory, token])
      
      // Neue Vorhersage für den ergänzten Text
      fetchNextTokenPrediction(newText)
    }, 800)
  }

  // Zurücksetzen auf original Text
  const resetToOriginal = () => {
    setCurrentText(text)
    setPredictionHistory([])
    fetchNextTokenPrediction(text)
  }

  return (
    <div className="w-full h-full flex flex-col">
      {/* Der aktuelle Text und Generierungs-Historie */}
      <div className="mb-4 p-3 bg-gray-50 rounded-lg border border-gray-200">
        <div className="font-medium mb-1 text-sm text-gray-600">Aktueller Text:</div>
        <p className="font-medium">
          {text}
          {predictionHistory.map((token, i) => (
            <span key={i} className="text-green-600 font-bold">{token}</span>
          ))}
        </p>
        
        {predictionHistory.length > 0 && (
          <div className="mt-2 pt-2 border-t border-gray-200 flex gap-2 items-center flex-wrap">
            <span className="text-xs text-gray-500">Token-Kette:</span>
            {predictionHistory.map((token, i) => (
              <span key={i} className="px-2 py-0.5 bg-green-100 text-green-800 text-xs rounded">
                {token.replace(/ /g, '␣')}
              </span>
            ))}
            
            <Button 
              variant="ghost" 
              size="sm"
              className="ml-auto text-xs h-7"
              onClick={resetToOriginal}
            >
              Zurücksetzen
            </Button>
          </div>
        )}
      </div>
      
      {/* Loading-Zustand */}
      {loading && (
        <div className="flex-1 flex flex-col items-center justify-center space-y-4">
          <p>Berechne Wahrscheinlichkeiten für den nächsten Token...</p>
          <div className="w-full max-w-md space-y-2">
            <Skeleton className="h-4 w-full" />
            <Skeleton className="h-8 w-3/4" />
            <Skeleton className="h-8 w-5/6" />
            <Skeleton className="h-8 w-2/3" />
            <Skeleton className="h-8 w-1/2" />
          </div>
        </div>
      )}
      
      {/* Fehlerzustand */}
      {!loading && error && (
        <div className="flex-1 flex items-center justify-center">
          <div className="text-red-500 text-center p-6 bg-red-50 rounded-lg">
            <p className="font-bold mb-2">Fehler bei der Token-Vorhersage:</p>
            <p>{error}</p>
            <Button 
              variant="outline" 
              className="mt-4" 
              onClick={() => fetchNextTokenPrediction(currentText)}
            >
              <ReloadIcon className="mr-2 h-4 w-4" />
              Erneut versuchen
            </Button>
          </div>
        </div>
      )}
      
      {/* Erfolgsfall - Wahrscheinlichkeiten und Auswahlmöglichkeiten */}
      {!loading && !error && topTokens.length > 0 && (
        <div className="flex-1 flex flex-col">
          {/* API-Hinweis, falls vorhanden */}
          {apiNotice && (
            <div className="mb-4 p-2 bg-amber-50 border border-amber-200 rounded text-sm text-amber-800">
              <strong>Hinweis:</strong> {apiNotice}
            </div>
          )}
          
          <div className="mb-4">
            <div className="flex justify-between items-center mb-2 flex-wrap gap-2">
              <h3 className="font-medium">Top Wahrscheinlichkeiten für den nächsten Token</h3>
              
              <Button
                variant="secondary"
                size="sm"
                onClick={selectRandomToken}
                disabled={!!selectedToken}
                className="flex items-center gap-1"
              >
                <ShuffleIcon className="w-3 h-3" />
                <span>Zufälliger Token</span>
              </Button>
            </div>
            
            <div className="space-y-2 mb-6">
              {topTokens.map((token, index) => (
                <motion.div
                  key={`${token.token}-${index}`}
                  initial={{ opacity: 0, y: 10 }}
                  animate={{ opacity: 1, y: 0 }}
                  transition={{ delay: index * 0.1 }}
                  className={`flex items-center gap-2 ${
                    selectedToken === token.token ? 'bg-green-50 rounded-lg p-2 ring-1 ring-green-200' : ''
                  }`}
                >
                  <div className="w-16 text-right font-mono text-sm">
                    {(token.probability * 100).toFixed(1)}%
                  </div>
                  
                  <Progress 
                    value={token.probability * 100} 
                    className={`h-6 ${
                      selectedToken === token.token ? 'bg-green-100' : ''
                    }`}
                  />
                  
                  <div className="font-semibold min-w-20 px-2">
                    "{token.token.replace(/ /g, '␣')}"
                  </div>
                  
                  <Button
                    variant="outline"
                    size="sm"
                    onClick={() => selectToken(token.token)}
                    disabled={!!selectedToken}
                    className="ml-auto"
                  >
                    Wählen
                  </Button>
                </motion.div>
              ))}
              
              {/* Visualisierung der "Long Tail" - alle anderen möglichen Tokens */}
              <motion.div
                initial={{ opacity: 0, y: 10 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ delay: topTokens.length * 0.1 }}
                className="flex items-center gap-2 opacity-70"
              >
                <div className="w-16 text-right font-mono text-sm text-gray-500">
                  {(remainingProbability * 100).toFixed(1)}%
                </div>
                
                <Progress 
                  value={remainingProbability * 100} 
                  className="h-6 bg-gray-100"
                />
                
                <div className="font-medium text-gray-600 px-2">
                  Tausende weitere mögliche Tokens...
                </div>
              </motion.div>
            </div>
          </div>
          
          {/* Ausgewählter Token */}
          <AnimatePresence>
            {selectedToken && (
              <motion.div
                initial={{ opacity: 0, height: 0 }}
                animate={{ opacity: 1, height: 'auto' }}
                exit={{ opacity: 0, height: 0 }}
                className="border-t border-gray-200 pt-4 text-center"
              >
                <p className="mb-2">Ausgewählter nächster Token:</p>
                <div className="text-xl font-bold bg-green-100 px-4 py-2 rounded-lg inline-block">
                  {selectedToken.replace(/ /g, '␣')}
                </div>
                <div className="mt-4 text-sm text-gray-600 flex items-center justify-center gap-2">
                  <span>Füge Token zum Text hinzu</span>
                  <ChevronRightIcon className="animate-pulse" />
                </div>
              </motion.div>
            )}
          </AnimatePresence>
        </div>
      )}
      
      {/* Fallback, wenn keine Tokens geladen wurden */}
      {!loading && !error && topTokens.length === 0 && (
        <div className="flex-1 flex items-center justify-center">
          <div className="text-center p-6 bg-gray-50 rounded-lg max-w-md">
            <p className="mb-4 text-gray-700">
              {apiNotice || "Keine Token-Vorhersagen vom Modell erhalten."}
            </p>
            <div className="flex justify-center gap-2">
              <Button
                onClick={() => fetchNextTokenPrediction(currentText)}
                className="flex items-center gap-1"
              >
                <ReloadIcon className="w-3 h-3" />
                Erneut versuchen
              </Button>
              {!useSimulation && (
                <Button
                  variant="outline"
                  onClick={() => {
                    const simulated = getSimulatedPredictions(currentText);
                    setTopTokens(simulated.topTokens);
                    setRemainingProbability(simulated.remainingProbability);
                    setApiNotice("Verwende simulierte Daten statt API-Ergebnissen");
                  }}
                >
                  Simulation verwenden
                </Button>
              )}
            </div>
          </div>
        </div>
      )}
    </div>
  )
}
```

## Schritt 3: Überprüfen und Anpassen der gpt2-Modell-Anfrage

Für englische Texte funktioniert gpt2 ausgezeichnet, aber für deutsche Texte sollten wir stattdessen ein mehrsprachiges Modell betrachten:

**src/app/api/predict-next/route.ts** (Modell aktualisieren)
```typescript
// Ändere das Modell zu einem mehrsprachigen Modell, das logprobs unterstützt
// Option 1: Ein mehrsprachiges GPT-Modell
const MODEL_ID = "bigscience/bloom-560m"; // Unterstützt Deutsch und gibt logprobs zurück

// Alternativ:
// const MODEL_ID = "dbmdz/german-gpt2"; // Deutsches GPT-2 Modell
```

## Schritt 4: Aktualisierte Next-Token-Seite mit deutschsprachigen Beispielen

**src/app/next-token/page.tsx** (aktualisieren)
```tsx
// Aktualisieren Sie die Beispiel-Buttons:
<Button
  variant="outline"
  onClick={() => handleTextChange("Die Zukunft der künstlichen Intelligenz")}
>
  Beispiel 1
</Button>

<Button
  variant="outline"
  onClick={() => handleTextChange("In der Schule lernen wir")}
>
  Beispiel 2
</Button>
```

## Wichtige Anpassungen

1. **Modellwechsel**: Wir verwenden jetzt ein Modell, das logprobs unterstützt (GPT-2 oder mehrsprachige Varianten).

2. **Verbesserte Fehlerbehandlung**: Die Komponente kann jetzt elegant mit leeren Token-Arrays umgehen und bietet klare Optionen zur Fehlerbehebung.

3. **Deutschsprachige Beispiele**: Die Beispielsätze wurden für deutschsprachige Benutzer optimiert.

4. **Flexible Simulation**: Ein Fallback-Mechanismus stellt sicher, dass die App funktional bleibt, selbst wenn das API-Modell keine guten Vorhersagen liefert.

5. **Bessere Validierung**: Der Code überprüft sorgfältig, ob logprobs in der Antwort enthalten sind und bietet informative Hinweise, wenn nicht.

Diese Änderungen sollten das Problem beheben und eine zuverlässige Next-Token-Prediction sowohl mit echter API als auch mit Fallback-Optionen ermöglichen.